\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage{xcolor}

\newcommand\inv{^{-1}}   
\newcommand{\Z}{\mathbb Z}
\newcommand{\R}{\mathbb R}
\newcommand{\Q}{\mathbb Q}
\newcommand{\C}{\mathbb C}
\newcommand{\N}{\mathbb N}
\newcommand{\quat}{\mathbb H}
\newcommand{\gal}{\text{Gal }}
\newcommand{\indep}{\perp \!\!\! \perp}

\begin{document}
\pagecolor{black}
\color{white}

\noindent{\bf Random Experiment}

    A procedure with several possible outcomes.

\medskip
\noindent{\bf Sample Space}

    The set of all possible outcomes of a random experiment.

\medskip
\noindent{\bf Sample Point}

    An individual possible outcome from a random experiment.

\medskip
\noindent{\bf Event}
    
    A subset of the sample space.
    
\medskip
\noindent{\bf Probability (Kolmogorov Definition)}

    Let $S$ be a sample space. A probability $\mathbb P$ is a function that assigns to each event $E$ a positive number with the following properties:

\medskip
\noindent{\bf Axioms of Probability}

    1. $0 \leq \mathbb P(E) \leq 1$
    
    2. $\mathbb P(S) = 1$
    
    3. If $E_1, E_2, E_3, \hdots$ are events with $E_i \cap E_j = \emptyset$ for all $i \neq j$, then \\
    $\mathbb P(E_1 \cup E_2 \cup \hdots) = \mathbb P(E_1) + \mathbb P(E_2) + \hdots$

\medskip
\noindent{\bf Probability Space}

    A pair $(S, \mathbb P)$, where $S$ is a sample space and $\mathbb P$ is a probability.

\medskip
\noindent{\bf Complement}

    Let $E$ be an event. Then the complement of $E$, denoted $\overline E$ or $E^\text c$, is the set which contains all elements in $S$ which are not in $E$.
    
    $\mathbb P(\overline E) = 1 - \mathbb P(E)$.

\medskip
\noindent{\bf Discrete Uniform Distribution}

    Suppose $|S| = n$, so $S=\{x_1,x_2,\hdots,x_n\}$. The discrete uniform distribution is the probability $\mathbb P$ which assigns to each simple event the number $\frac{1}{n}$.

    Thus, $\mathbb P(E) = \frac{|E|}{|S|}$.

\medskip
\noindent{\bf Permutation}

    Assume you have $n$ items. The ways these items can be arranged in order are called permutations.
    
    There are $\frac{n!}{(n-k)!}$ permutations of $n$ unique items.
    
\medskip
\noindent{\bf Choose}

    Without caring about order, the number of ways to choose $k$ items from a group of $n$ items is $n \choose k$.

    ${n \choose k} = \frac{n!}{k!(n-k)!}$.

\medskip
\noindent{\bf Conditional Probability}

    Let $(S,\mathbb P)$ be a probability space, and $A,B$ be two events with $\mathbb P(B) > 0$.
    
    The conditional probability of $A$ given $B$, denoted $\mathbb P(A|B)$, is defined as $\mathbb P(A|B) = \frac {\mathbb P(A \cap B)}{\mathbb P(B)}$.

\newpage
\noindent{\bf Independence}

    $A$ and $B$ are independent, denoted $A \indep B$ if $\mathbb P(A \cap B) = \mathbb P(A) \mathbb P(B)$.
    
    (Also true for more than 2 events)

\medskip
\noindent{\bf Random Variable}

    A random variable is a function which assigns a real number to each sample point:
    
        $X:S \to \mathbb R$
        
        ($X(\text{sample point}) = \#$)
        
        Notation: $\mathbb P(X = k) = p_X(k)$.

\medskip
\noindent{\bf Probability Mass Function (Distribution Function)}

    $p_X(k):\mathbb R \to [0,1]$ is called the probability mass function.
    
    Note that $0 \leq p_X(k) \leq 1$.
    
    $\sum_kp_X(k)=1$.
    
    (Assuming that $X$ takes a countable number of possible values.)
    
    When $X$ has a countable number of possible values, it is called a discrete random variable.
    
\medskip
\noindent{\bf Binomial Random Variable}

    Let $X = \#$ of successes in $n$ independent trials of an experiment with two possible outcomes. Let $p$ denote the probability of success in each independent trial. (often $q=1-p$ denotes the probability of failure) Then $X$ is a binomial random variable if $p_X(k) = \mathbb P(X=k) = {n \choose k}p^k(1-p)^{n-k}$ for all integers $0 \leq k \leq n$.

    Example: Let $X$ be a binomial random variable with parameters $n$ and $p$. (Denoted $X \sim \text{Binomial}(n,p)$, where $\sim$ means ``is distributed as")
    
    The PMF of $X$ is then $p_X(k) = {n \choose k}p^k(1-p)^{n-k}$. ($0 \leq n \leq k$) This is called the binomial distribution.

\medskip
\noindent{\bf Geometric Random Variable}

    Let $X = \#$ of independent trials of an experiment with two possible outcomes up to and including the first success. Then, $X$ is a geometric random variable wih parameter $p$, denoted $X \sim$ Geometric$(p)$.
    
    $p_X(k) = \mathbb P(X = k) = (1 - p)^{k - 1}p$.
    
\medskip
\noindent{\bf Bayes' Theorem}

    $\mathbb P(A|B) = \frac{\mathbb P(B|A)\mathbb P(A)}{\mathbb P(B)}$.
    
\medskip
\noindent{\bf Expected Value}

    Let $X:S \to I = \{x_1, x_2, \hdots\} \subseteq \mathbb R$ be a discrete random variable. The expected value of $X$, denoted $\mathbb E[X] = \mathbb EX$, is defined as $$\mathbb EX = \sum_{x_k \in I}\mathbb P(X=x_k)\cdot x_k.$$
    
    If $Y$ is a binomial random variable with parameters $n, p$, then $\mathbb E[Y] = np$.
    
    If $Z$ is a geometric random variable with paremeter $p$, then $\mathbb E[Z] = \frac1p$.
    
\medskip
\noindent{\bf Linearity of Expectation}

    Let $X_1, X_2, \hdots, X_n$ be random variables and $\alpha_1, \alpha_2, \hdots, \alpha_n \in \mathbb R$.
    
    Then define $X = \alpha_1X_1 + \alpha_2X_2 + \hdots + \alpha_nX_n = \sum_{k=1}^n\alpha_kX_k$.
    
    Then, $\mathbb EX = \mathbb E[\alpha_1X_1 + \hdots + \alpha_nX_n] = \alpha\mathbb EX_1 + \hdots + \alpha_n \mathbb EX_n$.

\medskip
\noindent{\bf A Helpful Trick}

    When you're trying to do a complicated expected value, try breaking up the complicated variable into a sum of many simple variables. (Hat check problem, sock problem)

\medskip
\noindent{\bf Variance}

    Let $X:S \to \mathbb R$ be a discrete random variable. The variance of $X$, denoted $\mathbb V[X]$, is a number describing the spread of possible values: $$\mathbb V[X] = \mathbb E[(X - \mathbb E[X])^2].$$
    
    The standard deviation is given by $$\sqrt{\mathbb V[X]}.$$ This will sometimes be denoted $\sigma_X$.
    
    To compute variance directly: $$\mathbb V[X] = \sum_k \mathbb P(X = k) \cdot (k - \mathbb E[X])^2.$$
    
    But the actual way you will calculate variance is this: $$\mathbb V[X] = \mathbb E[X^2] - (\mathbb E[X])^2.$$
    
\medskip
\noindent{\bf Variance of Geometric Random Variable}

    Let $Y$ be a geometric random variable with parameter $p$. Then, $$\mathbb V[Y] = \frac{1-p}{p^2}.$$
    
\medskip
\noindent{\bf Variance of Binomial Random Variable}

    Let $X$ be a binomial random variable with parameters $n, p$. Then, $$\mathbb V[X] = np(1-p).$$
    
\medskip
\noindent{\bf Independence of Random Variables}

    Let $X,Y: S \to \mathbb R$ be discrete random variables. We say $X$ and $Y$ are independent if $\mathbb P(X=a~\text{and}~Y=b) = \mathbb P(X=a) \mathbb P(Y = b)$ for all $a,b \in \mathbb R$.
    
\medskip
\noindent{\bf Breaking up sums of independent random variables in variances}

    If $X$ and $Y$ are independent, then $\mathbb V[X+Y]=\mathbb V[X] + \mathbb V[Y]$.
    
\medskip
\noindent{\bf Breaking up products of independent random variables in expected values}

    If $X$ and $Y$ are independent, then $\mathbb E[XY] = \mathbb E[X] \mathbb E[Y]$.
    
\medskip
\noindent{\bf Scaling and Shift}

    Let $a \in \mathbb R$ and $X$ be a discrete random variable. Then, \begin{align*}
        \mathbb V[aX]    &= a^2\mathbb VX \\
        \mathbb V[a + X] &= \mathbb VX
    \end{align*}
    
\medskip
\noindent{\bf Markov's Inequality}

    Let $X$ be a nonnegative random variable. Then, $\mathbb P(X \geq a) \leq \frac{\mathbb EX}{a}$ for all $a > 0$.
    
\medskip
\noindent{\bf Chebyshev's Inequality}

    Let $X$ be a         random variable. Then, $\mathbb P(|X - \mathbb EX| \geq a) \leq \frac{\mathbb VX}{a^2}$ for all $a > 0$.
    
\medskip
\noindent{\bf Weak Law of Large Numbers}

    Let $X_1, X_2, \hdots$ be independent, identically-distributed random variables. Then, for all $\epsilon > 0$,
    $$\lim_{n\to\infty}\mathbb P\bigg{(}\bigg{|}\frac{X_1+X_2 + \hdots + X_n}{n} - \mathbb E[X_n] \bigg{|} \geq \epsilon\bigg{)} = 0.$$
    
    In other words, the average of the first $n$ trials of an experiment approach the expected value of the $n^\text{th}$ trial in the long run.
    
\medskip
\noindent{\bf Poisson Random Variable}

    Let $X$ be the number of events which occur in a fixed time interval $(0, T)$.
    
    If $X$ satisfies:
    
    1. The number of events in two disjoint subintervals are independent.
    
    2. The number of successes per unit of time, usually referred to as the rate $\lambda$, is independent of time.
    
    3. In an infinitesimal interval, $[t, t+\delta t)$, there is at most one event.
    
    then $X$ is a Poisson distributed random variable with parameters $\lambda, T$.
    
    $\mathbb P(X=k) = \frac{e^{-\lambda T}(\lambda T)^k}{k!}$ for $k \in \mathbb N_0$.
    
    $\mathbb EX = \lambda T$.
    
    $\mathbb VX = \lambda T$.
    
    Sometimes, since $\lambda$ and $T$ are always a product, this is shortened to having only one parameter, $\lambda$, which is actually $\lambda T$.
    
\medskip
\noindent{\bf Hypergeometric Random Variable}

    Suppose you have $N$ items of which $K$ are special. From $N$ total items, you pick $n$ uniformly at random. Let $X$ by the number of special items picked. Then, $X$ is said to be a hypergeometric random variable with parameters $N,K,n$.
    
    $\mathbb P(X=k) = \frac{{K \choose k}{{N-K}\choose{n-k}}}{{N \choose n}}$ for $\max\{0, n + K - N\} \leq k \leq \min\{K,n\}$
    
\medskip
\noindent{\bf Fact about continuous random variables}

    If $X: S \to \mathbb R$ is continuous, then $\mathbb P(X = x) = 0$ for all $x \in \mathbb R$.
    
\medskip
\noindent{\bf Cumulative Distribution Function}

    The cumulative distribution function of a random variable $X$ is the function $F(x) = \mathbb P(X \leq x)$.
    
    Every CDF is non-decreasing.
    
    The CDF of a discrete random variable is a step function.
    
    $\mathbb P(a \leq X \leq b) = F(b) - F(a)$ for any $a < b$.
    
    $\lim\limits_{x\to\infty} F_X(x) = 1$, and $\lim\limits_{x\to-\infty} F_X(x) = 0$.
    
\medskip
\noindent{\bf Continuous Random Variable}

    A random variable is continuous if its CDF is continuous.
    
\medskip
\noindent{\bf Probability Density Function (PDF)}

    The probability density function of a continuous random variable $X$ with CDF $F_X(x)$ is the function $$f(x) = \frac{d}{dx} F_X(x)$$ for all $x$ such that $F_X(x)$ is differentiable.
    
    $$f_X(x) \geq 0$$
    
    $$\int\limits_{-\infty}^\infty f_X(x)dx = 1$$
    
    $$\int\limits_a^bf_X(x)dx = F_X(b) - F_X(a) = \mathbb P(a < X \leq b)$$
    
\medskip
\noindent{\bf Expected value and variance of a continuous random variable}

    Let $X$ be a continuous random variable with PDF $f_X$. The expected value of $X$ is given by 
    $$\mathbb EX = \int\limits_{-\infty}^\infty f_X(x) \cdot x\,dx$$
    
    The variance of $X$ is given by the same formula as before. All previously-stated properties of expectation and variance still hold.
    
\medskip
\noindent{\bf CDF of Geometric Random Variables}

    $$1 - (1-p)^{\lfloor x \rfloor}$$
    
\medskip
\noindent{\bf Uniform Distribution}

    A random variable $X$ is uniformly distributed on $(\Theta_1, \Theta_2)$, denoted $$X \sim \text{Uniform}(\Theta_1, \Theta_2),$$ if density is given by $$f_X(x) = \begin{cases} \frac1{\Theta_2 - \theta_1} & \Theta_1 < x < \Theta_2, \\ 0 & \text{else.} \end{cases}$$
    
    Then, $$F_X(x) = \begin{cases} 0 & x < \Theta_1, \\ \frac{x - \Theta_1}{\Theta_2 - \Theta_1} & \Theta_1 \leq x \leq \Theta_2, \\ 0 & x > \Theta_2. \end{cases}$$

    If $(c,d)$ is a subinterval of $(\Theta_1, \Theta_2)$, then $\mathbb P(c < X < d) = \int\limits_c^d \frac1{\Theta_2 - \Theta_1}\,dx = \frac{d - c}{\Theta_2 - \Theta_1}$. This is also obvious.
    
    $$\mathbb EX = \frac{\Theta_1 + \Theta_2}{2}.$$
    
    $$\mathbb VX = \frac1{12}(\Theta_2 - \Theta_1)^2.$$

\medskip
\noindent{\bf Normal (Gaussian) Distribution}

    We say $X$ is a Gaussian (normal) random variable if its density (PDF) is given by $$f_X(x) = \frac1{\sqrt{2\pi\sigma^2}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}.$$
    
    We say $X \sim \text{ Normal}(\mu, \sigma^2)$.
    
    $\mathbb EX = \mu$.
    
    $\mathbb VX = \sigma^2$.
    
    If $X \sim \text{ Normal}(\mu_X,\sigma_X^2)$ and $Y \sim \text{ Normal}(\mu_Y,\sigma_Y^2)$ are independent, then $X+Y = \text{ Normal}(\mu_X + \mu_Y, \sigma^2_X + \sigma^2_Y)$.
    
    $aX+b \sim \text{ Normal}(a\mu_X+b, a^2\sigma_X^2)$.
    
    If you combine many sources of randomness, you get a normal distribution (Central Limit Theorem)
    
    Normal distribution maximizes entropy (???)
    
    If you don't know anything about a random variable, Normal is a decent guess.
    
    $\sim68\%$ of probability is within one standard deviation. 

    $\sim95\%$ for two standard deviations.
    
    $\sim99.7\%$ for three.
    
\medskip
\noindent{\bf Standard Normal Distribution}

    If $X$ is a Gaussian random variable, then $$Z = \frac{X-\mu_X}{\sigma_X} \sim \text{ Normal}(\mu=0, \sigma^2=1)$$ is a standard normal random variable. To compute probabilities for $X$, convert to probabilities involving $Z$ and use precomputed values for standard normal.

\medskip
\noindent{\bf Exponential Random Variable}

    The random variable $X$ is exponentially distributed with parameter $\lambda > 0$, if its density is $$f_X(x) = \begin{cases} \lambda e^{-\lambda x} & x > 0, \\ 0 & \text{else.} \end{cases}$$
    
    Then, its CDF is $$F_X(x) = \begin{cases} 1 - e^{-\lambda x} & x \geq 0, \\ 0 & x < 0. \end{cases}$$
    
    Sometimes parameterized using $\beta = \frac1\lambda$.
    
    $X$ is the waiting time until first event when events occur at rate $\lambda$ per unit time.
    
    This is kind of like the continuous version of the geometric distribution.
    
    $$\mathbb EX = \frac1\lambda$$

    $$\mathbb VX = \left(\frac1\lambda\right)^2$$
    
    The exponential distribution is the only continuous distribution with memorylessness: 
    $$\mathbb P(X > t + s~|~X > t) = \mathbb P(X > s)$$
    
    In other words, if no event occurs by time $t$, the probability of the event occurring $s$ time units in the future is the same as the probability of it occurring after waiting $s$ from the outset. 

\medskip
\noindent{\bf Gamma Random Variable}

    $X$ is said to be Gamma distributed with parameters $r \in \mathbb N$ and $\lambda > 0$ if its density is $$f_X(x) = \begin{cases} \frac{\lambda^r}{(r-1)!}x^{r-1}e^{-\lambda x} & x > 0, \\ 0 & \text{else.} \end{cases}$$

    $X$ is the waiting time until $r$ events have occurred when events occur at a constant rate $\lambda$.

    
    $$\mathbb VX = \frac r {\lambda^2}$$
    
\medskip
\noindent{\bf Joint Distribution for Discrete Random Variables}

    Let $X: S \to \{x_1, x_2, x_3, \hdots\}$ and $Y: S \to \{y_1, y_2, y_3, \hdots\}$ be discrete random variables on the same sample space $S$. The joint distribution of the random vector $(X, Y)$ is given by $$\mathbb P(X = x_k, Y=y_l),$$ sometimes denoted $p_{X,Y}(x_k,y_l)$.
    
\medskip
\noindent{\bf CDF of a Random Vector}

    Let $X,Y:S\to\mathbb R$ be random variables. The CDF of the random vector $(X,Y)$ is given by $$F_{X,Y}(x, y) = \mathbb P(X\leq x, Y\leq y).$$

\medskip
\noindent{\bf Joint Distribution for Continuous Random Variables}

    Let $X,Y:S \to \mathbb R$ be continuous random variables. The joint distribution or joint density of $(X,Y)$ is the function $$f_{X,Y}(x, y) = \frac{\delta^2}{\delta x \delta y} F_{X,Y}(x,y).$$
    
    $$f_{X,Y}(x,y) \geq 0\,\forall x,y \in \mathbb R$$
    
    $$\int\limits_{-\infty}^\infty \int\limits_{-\infty}^\infty f_{X,Y}(x,y)\,dydx = 1$$

\medskip
\noindent{\bf Marginal Distribution of Discrete a Random Variable}

    Let $X, Y$ be discrete random variables with joint distribution $p_{X,Y}(x,y)=\mathbb P(X=x, Y=y)$. The marginal distribution of $Y$, denoted $p_Y(y)$, is defined by $$p_X(x) = \sum_{y}p_{X,Y}(x,y).$$

\medskip
\noindent{\bf Conditional Distribution of Discrete a Random Variable}

    The conditional distribution of $X$ given $Y=y$ is $$p_{X|Y}(x) = \frac{p_{X,Y}(x,y)}{p_Y(y)}.$$

\medskip
\noindent{\bf Independence for Joint Distributions of Discrete Random Variables}

    $X$ and $Y$ are independent if and only if $p_{X,Y}(x,y) = p_X(x)p_Y(y)$ for all $x,y$.

\medskip
\noindent{\bf Marginal Distribution of a Continuous Random Variable}

    Let $X,Y$ be continuous random variables with density $f_{X,Y}(x,y)$. The marginal distribution of $X$ is $$f_X(x) = \int\limits_{-\infty}^\infty f_{X,Y}(x,y)\,dy,$$ and the marginal distribution of $Y$ is $$f_Y(y) = \int\limits_{-\infty}^\infty f_{X,Y}(x,y)\,dx.$$

\medskip
\noindent{\bf Conditional Distribution of Continuous Variables}

    The conditional distribution of $X$ given $Y=y$ is $$p_{X|Y}(x) = \frac{f_{X,Y}(x,y)}{f_Y(y)}.$$

\medskip
\noindent{\bf Independence for Joint Distributions of Continuous Random Variables}

    $X$ and $Y$ are independent if and only if $f_{X,Y}(x,y) = f_X(x)f_Y(y)$ for all $x,y \in \mathbb R$.

\medskip
\noindent{\bf Expected Value of a Function of Discrete Random Variables}

    Let $(X,Y)$ be a discrete random vector and $p_{X,Y}(x,y)$ their joint distribution. Let $$g(x,y) \to \mathbb R$$ be a function. Then, $$\mathbb E[g(X,Y)] = \sum_x\sum_yp_{X,Y}(x,y)g(x,y).$$

\medskip
\noindent{\bf Expected Value of a Function of Continuous Random Variables}

    Let $(X,Y)$ be a continuous random vector and $p_{X,Y}(x,y)$ their joint distribution. Let $$g(x,y) : \mathbb R^2 \to \mathbb R$$ be a function. Then, $$\mathbb E[g(X,Y)] = \int\limits_{-\infty}^\infty \int\limits_{-\infty}^\infty f_{X,Y}(x,y)g(x,y)\,dy\,dx.$$

\medskip
\noindent{\bf Covariance}

    Let $X,Y$ be random variables. The covariance of $X$ and $Y$ is given by
    \begin{align*}
        \text{Cov}(X,Y) &= \mathbb E[(X-\mathbb EX)(Y-\mathbb EY)] \\
                        &= \mathbb E[XY] - (\mathbb EX)(\mathbb EY).
    \end{align*}

\medskip
\noindent{\bf Correlation Coefficient}

    Let $X,Y$ be random variables. The correlation coefficient of $X$ and $Y$, denoted $\rho_{XY}$ or $\rho(X,Y)$ is given by $$\frac{\text{Cov}(X,Y)}{\sqrt{\mathbb VX}\sqrt{\mathbb VY}}.$$ Note that Cauchy-Schwarz guarantees that a correlation coefficient is between -1 and 1.

\medskip
\noindent{\bf Estimation/Inference}

    Let $x_1, \hdots, x_n$ be the observed outcomes of $n$ independent, identically-distributed random variables $X_1, \hdots, X_n$.

    The collection of observations $x_1, \hdots, x_n$ of $X_1, \hdots, X_n$ are referred to as a sample. The process of identifying unknown parameters of $X_i$ using a sample is estimation or inference.
 
 \medskip
 \noindent{\bf Point Estimator}
    
    A point estimator is a random variable $\hat\theta$ defined using the sample $X_1, \hdots, X_n$ which is an estimate for the true parameter $\theta$ of $X_i$. The bias of a point estimator $\hat\theta$ for a parameter $\theta$ is given by $$\text{Bias}(\hat\theta) = \mathbb E[\hat\theta] - \theta.$$ $\hat\theta$ is called unbiased if its bias is 0.

\medskip
\noindent{\bf Sample Mean}

    Let $X_1, \hdots X_n$ be independent, identically-distributed random variables such that their means are equal. Then, the sample mean is defined by $$\hat\mu = \frac1n\sum_{i=1}^nX_i.$$

\medskip
\noindent{\bf Unbiased Sample Variance}

    Let $X_1, \hdots X_n$ be independent, identically-distributed random variables such that their variances are equal. Then, the unbiased sample variance is defined by $$\hat{\sigma^2} = \frac1{n-1}\sum_{i=1}^n(X_i-\overline X)^2.$$

\medskip
\noindent{\bf Method of Moments}

    Let $X$ be a random variable and $X_1, \hdots, X_n$ be independent, identically-distributed samples with the same distribution as $X$. The $k^\text{th}$ moment of $X$ is defined as $\mathbb E[X^k]$. The $k^\text{th}$ sample moment is $\frac1n \sum_{i=1}^nX_i^k$.

    To estimate $k$ unknown parameters, set the first $k$ moments equal to the first $k$ sample moments and cross your fingers.

\medskip
\noindent{\bf Mean-squared error}

    Let $\Theta$ be a parameter of interest and $\hat\Theta$ be a random variable in terms of $n$ independent, identically-distributed random variables be our guess for $\Theta$. The mean-squared error (MSE) of $\hat\Theta$ is given by \[\text{MSE}(\hat\Theta)=  \mathbb E[(\hat\Theta - \Theta)^2].\] This is useful because then negative and positive errors don't cancel each other out.

    Property: \[\text{MSE}(\hat\Theta) = \mathbb V[\hat\Theta] + (\text{Bias}(\hat\Theta))^2.\]

\medskip
\noindent{\bf Consistency}

    Let's say we have $n$ independent, identically-distributed random variables, where $n$ can vary. Then, $\hat\Theta$ is called consistent if for any $\epsilon > 0$, \[\lim_{n\to\infty} \mathbb P(|\hat\Theta - \Theta| > \epsilon) = 0.\]

    Equivalently, an estimate $\hat\Theta$ is consistent if it is unbiased and $\mathbb V[\hat\Theta] \to 0$ as $n \to \infty$. (usually)

\medskip
\noindent{\bf Likelihood Function}

    Let $X_1, \hdots, X_n$ be independent, identically-distributed samples of a distribution with $k$ unknowns $\Theta_1, \hdots, \Theta_k$. Let $x_1, \hdots, x_n$ be the observed values of $X_1, \hdots, X_n$. The likelihood function of the parameters is given by
    $$\mathcal L(\Theta_1, \hdots, \Theta_k) = p_{X_1, \hdots, X_n}(x_1, \hdots, x_n~|~\Theta_1, \hdots, \Theta_k)$$ if $X_i$ are discrete, and
    $$\mathcal L(\Theta_1, \hdots, \Theta_k) = f_{X_1, \hdots, X_n}(x_1, \hdots, x_n~|~\Theta_1, \hdots, \Theta_k)$$ if $X_i$ are continuous.

    These are not conditional probabilities that we would need to use the formula for. They're just reminders that we use the $\Theta$s to compute the probabilities (densities).

\medskip
\noindent{\bf Method of Maximum Likelihood}

    Choose as estimates the values of $\Theta_1, \hdots, \Theta_k$ which maximize $\mathcal L(\Theta_1, \hdots, \Theta_k)$. In other words, our guess should be whichever parameters would have led to the highest probability of observing our results.

    Note: To maximize $\mathcal L$, it's usually easier to take the log and maximize that (allowed because log is monotic).

    Note: If the samples are i.i.d., we can split up the likelihood function into a product of probabilities (densities).

\medskip
\noindent{\bf The Central Limit Theorem}

    Let $X_1, \hdots, X_n$ be i.i.d. If $n$ is large, then $\overline X = \frac1n\sum\limits_{i=1}^nX_i$ is approximately normal.

    Note: This means that if $\mu = \mathbb EX_i$ and $\sigma^2 = \mathbb VX_i$, then $\overline X \sim \text{Normal}(\mu, \frac{\sigma^2}n)$ for large $n$.

    A typical value of ``large" is 30.

    More precisely, \[\frac{\overline X - \mu}{\frac{\sigma}{\sqrt n}} \sim \text{Normal}(0,1).\]

    As a consequence, \[\overline X \sim \text{Normal}\left(\mu, \frac{\sigma^2}{n}\right)\]

    Another form: If $\mu = \mathbb EX_i$, and $\sigma^2 = \mathbb VX_i$, CLT implies that $\overline X \sim \text{Normal}(\mu, \frac{\sigma^2}n)$ for large $n$.

    Further, \[\mathbb P\left(-z_{\frac\alpha2} \leq Z \leq z_{\frac\alpha2}\right) = 1 - \alpha,\] where $z_x$ is defined such that $\mathbb P(Z \geq z_x) = x$.

\medskip
\noindent{\bf Confidence Interval for a Mean with a Known Standard Deviation}

    A $1 - \alpha$ confidence interval for $\mu$ if sample size $n$ is large:
    \[\left(\overline X - z_{\frac\alpha2}\frac\sigma{\sqrt n}, \overline X + z_{\frac\alpha2}\frac\sigma{\sqrt n}\right)\]
    If we don't know $\sigma$, and $n$ is large, it's common (dirty trick) to replace it with $$s = \frac1{n-1}\sum_{i=1}^n(X_i - \overline X)^2$$

    This is what you should do if you know $\sigma$ and not $\mu$.

\medskip
\noindent{\bf $t$-Distribution}

    Let $X_1, X_2, \hdots, X_n$ be i.i.d. Normal$(\mu, \sigma^2)$ R.V.s.

    Let $$T = \frac{\overline X - \mu}{\frac s{\sqrt n}}.$$

    Then $T$ is said to be $t$-distributed with $n-1$ degrees of freedom.

    Note: If samples are approximately normally distributed, or the sample size is large enough for the CLT to apply, then this is a good approach.

    Note: As $n$ increases, the $t$-distribution with $n$ degrees of freedom gets closer to the standard normal.

\medskip
\noindent{\bf Confidence Interval when Standard Deviation is Unknown}

    This is what you should do if you don't know $\sigma$ or $\mu$.
    $$\mathbb P\left(\overline X - \frac s{\sqrt n} t_{n-1, \frac\alpha2} \leq \mu \leq \overline X + \frac s{\sqrt n} t_{n-1, \frac\alpha2}\right) = 1 - \alpha$$

    To construct a confidence interval with confidence $(1-\alpha)$, solve
    \[\mathbb P\left(\overline X - z_{\frac\alpha2}\frac\sigma{\sqrt n} \leq \overline X \leq \overline X + z_{\frac\alpha2}\frac\sigma{\sqrt n}\right) = 1 - \alpha.\]

\medskip
\noindent{\bf Statistic}

    A statistic is a random variable $T$ which is computed from a sample $X_1, \hdots, X_n$.

    Examples: Estimators, the median, the mode, the mean, the variance.

\medskip
\noindent{\bf Hypothesis Test}

    A hypothesis test is a procedure with four components: \\
    1. A null hypothesis $H_0$: A hypothesis you are trying to evaluate based on the data. Almost always a specific value, e.g. the coin is fair. \\
    2. An alternative hypothesis $H_a$: the hypothesis which we are looking for evidence for compared to the null hypothesis. Often a range of values, e.g. the coin is not fair. \\
    3. Atest statistic: A statistic to be used to evaluate both hypotheses. \\
    4. A rejection region: A set such that if the test statistic is in this region, we conclude that $H_0$ is false in favor of $H_a$. If the test statistic is not in this region, we fail to reject $H_a$.

\medskip
\noindent{\bf Type I Error}

    A type I error occurs when $H_0$ is true but it is rejected.

\medskip
\noindent{\bf Type II Error}

    A type II error occurs when $H_a$ is true but you fail to reject $H_0$.

\medskip
\noindent{Large Sample Test for Mean}

    Let $X_1, \hdots, X_n$ be i.i.d. with $\mu = \mathbb EX_i$ unknown and $\mathbb VX_i = \sigma^2$ unknown. (If $n$ is large and $\sigma^2$ is unknown, it's not too bad to just use $s^2$ instead.)

    Null hypothesis: $\mu = \mu_0$, (for testing fairness of a coin, $\mu_0 = .5$)

    Alternative hypothesis: One of the following: $\mu > \mu_0$, $\mu < \mu_0$, $\mu \neq \mu_0$.

    Test statistic: $\overline X$.

    Rejection region: We would like a type I error rate of $\alpha$, which we select.

\end{document}
